% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\section{Обзор}
\label{sec:relatedworks}
В данном разделе представлен обзор рекуррентных нейронных сетей (RNN) и её усовершенствованной архитектуры GRU (Gated Recurrent Unit). Рассмотрены ключевые принципы работы GRU, включая механизм управления потоками информации, а также её особенности и преимущества в задачах обработки последовательных данных.

\subsection{Обзор существующих решений}

Рекуррентные нейронные сети (RNN, Recurrent Neural Networks) были впервые предложены Дэвидом Румельхартом, Джеффри Хинтоном и Рональдом Уильямсом в 1986 году. Они разработали метод, позволяющий учитывать временные зависимости в последовательных данных с помощью скрытых состояний, которые сохраняют информацию о предыдущих шагах.

Одной из главных проблем стандартных RNN является \textbf{затухание градиентов}. Это явление связано с использованием цепного правила при обратном распространении ошибки: производные функции активации (например, сигмоида или гиперболический тангенс) уменьшаются на каждом шаге. При длинных последовательностях значения градиентов становятся настолько малыми, что веса перестают обновляться. Это затрудняет обучение зависимостей на больших временных промежутках. В некоторых случаях возможны и \textbf{взрывные градиенты}, когда значения градиентов увеличиваются экспоненциально, что делает обучение нестабильным.

Для частичного решения этой проблемы в 2014 году Кён Чо и его коллеги предложили архитектуру GRU (Gated Recurrent Unit). В её основе лежит использование двух гейтов: \textbf{обновляющего гейта} и \textbf{сбрасывающего гейта}, которые позволяют гибко управлять сохранением и сбросом информации.
\begin{enumerate}
\item \textbf{Обновляющий гейт} (\( z_t \)) отвечает за то, какую часть информации из предыдущего скрытого состояния следует сохранить:
  \[
  z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z),
  \]
  где \( W_z \) и \( b_z \) --- параметры обновляющего гейта, \( h_{t-1} \) --- скрытое состояние на предыдущем шаге, \( x_t \) --- входные данные, а \( \sigma \) --- сигмоида.

\item \textbf{Сбрасывающий гейт} (\( r_t \)) определяет, какую часть информации из предыдущего состояния следует игнорировать:
  \[
  r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r).
  \]

\item \textbf{Новое скрытое состояние} (\( h_t \)) вычисляется следующим образом:
\[
h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t,
\]
\item где \textbf{промежуточное состояние} (\( \tilde{h}_t \)) рассчитывается по формуле:
\[
\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h).
\]
\end{enumerate}

Такая структура позволяет GRU эффективно решать проблему затухания градиентов, сохраняя долгосрочные зависимости в данных. В отличие от архитектуры LSTM (Long Short-Term Memory), GRU имеет более простую структуру, поскольку использует меньше параметров и не включает отдельной ячейки памяти. Это делает GRU менее вычислительно затратной и удобной для использования в задачах, требующих обработки последовательных данных.

\subsection{Обзор используемых технологий}
Для реализации архитектуры GRU в рамках работе использованы встроенные слои рекуррентных нейронных сетей из библиотеки TensorFlow, такие как \textbf{tf.keras.layers.GRU}. Эти слои предоставляют реализацию гейтов и скрытых состояний, включая механизмы обновления и сброса, что позволяет сосредоточиться на настройке модели и анализе её поведения, а так же настройке гиперпараметров и экспериментах.

\subsection{Выводы}

Обзор существующих решений показывает, что GRU является эффективной и вычислительно выгодной архитектурой для обработки последовательных данных, решая основные ограничения RNN. Выбор TensorFlow и использование его встроенных слоёв позволяют минимизировать вероятность ошибок реализации и ускорить процесс экспериментов. Эти технологии дают основу для успешной интеграции GRU в проект DEGANN и исследования её свойств.
